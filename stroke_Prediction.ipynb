{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading Imports & the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from sklearn.metrics import f1_score\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from sklearn.calibration import LinearSVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the csv data file to a Pandas DataFrame\n",
    "dataset = pd.read_csv('/Users/ibrahimharoon/Documents/Uni/Year 3/Final Project/Stroke Prediction Tool/strokeDataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataset Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Finding the total rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the total rows and columns\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Identifying data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â Printing the first 5 rows of the dataset\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Checking for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if there are any missing values in each column\n",
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Checking for outliers in numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=dataset['avg_glucose_level']).set(title=\"Avg Glucose Level\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=dataset['bmi']).set(title=\"BMI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Checking if the distribution of stroke is balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'1' who had a stroke and '0' who did not have a stroke.\n",
    "dataset['stroke'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocessing 1: Imputation & Oversampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Creating a copy of the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetCopy = dataset.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Removing the id column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetCopy = datasetCopy.drop('id', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Binary encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetCopy['ever_married'] = datasetCopy['ever_married'].replace({'No' : 0, 'Yes' : 1})\n",
    "datasetCopy['Residence_type'] = datasetCopy['Residence_type'].replace({'Rural' : 0, 'Urban' : 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetCopy = pd.get_dummies(datasetCopy, columns=['gender', 'work_type', 'smoking_status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetCopy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Splitting the dataset into features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = datasetCopy.drop(columns='stroke', axis=1)\n",
    "y = datasetCopy['stroke']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Train - test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Train, X_Test, Y_Train, Y_Test = train_test_split(x, y, train_size=0.7, shuffle=True, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Replacing missing values with imputed values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = KNNImputer()\n",
    "imputer.fit(X_Train)\n",
    "\n",
    "X_Train = pd.DataFrame(imputer.transform(X_Train), index=X_Train.index, columns=X_Train.columns)\n",
    "X_Test = pd.DataFrame(imputer.transform(X_Test), index=X_Test.index, columns=X_Test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Oversampling/Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTE Oversampling\n",
    "smote = SMOTE(sampling_strategy=0.5, random_state=1)  # Adjust the ratio as needed\n",
    "X_Train_Oversampled, Y_Train_Oversampled = smote.fit_resample(X_Train, Y_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undersampling\n",
    "undersampler = RandomUnderSampler(sampling_strategy=0.5, random_state=1)  # Adjust the ratio as needed\n",
    "X_Train_Undersampled, Y_Train_Undersampled = undersampler.fit_resample(X_Train, Y_Train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Scaling values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "# Oversampled Set\n",
    "scaler.fit(X_Train_Oversampled)\n",
    "X_Train_Oversampled = pd.DataFrame(scaler.transform(X_Train_Oversampled), index=X_Train_Oversampled.index, columns=X_Train_Oversampled.columns)\n",
    "X_Test_Oversampled = pd.DataFrame(scaler.transform(X_Test), index=X_Test.index, columns=X_Test.columns)\n",
    "\n",
    "# Undersample Sey\n",
    "scaler.fit(X_Train_Undersampled)\n",
    "X_Train_Undersampled = pd.DataFrame(scaler.transform(X_Train_Undersampled), index=X_Train_Undersampled.index, columns=X_Train_Undersampled.columns)\n",
    "X_Test_Undersampled = pd.DataFrame(scaler.transform(X_Test), index=X_Test.index, columns=X_Test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking standard Deviation\n",
    "print(X_Train_Oversampled.std())\n",
    "print(X_Train_Undersampled.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"                   Logistic Regression\": LogisticRegression(class_weight='balanced'),\n",
    "    \"                   K-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"                         Decision Tree\": DecisionTreeClassifier(class_weight='balanced'),\n",
    "    \"                        Neural Network\": MLPClassifier(),\n",
    "    \"                         Random Forest\": RandomForestClassifier(class_weight='balanced'),\n",
    "    \"                     Gradient Boosting\": GradientBoostingClassifier(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the models\n",
    "for name, model in models.items():\n",
    "    \n",
    "    # Train on oversampled data\n",
    "    model.fit(X_Train_Oversampled, Y_Train_Oversampled)\n",
    "    \n",
    "    # Predictions for oversampled test set\n",
    "    oversampled_test_preds = model.predict(X_Test_Oversampled)\n",
    "    oversampled_test_probs = model.predict_proba(X_Test_Oversampled)[:, 1]  # Probability estimates\n",
    "    \n",
    "    # Train on undersampled data\n",
    "    model.fit(X_Train_Undersampled, Y_Train_Undersampled)\n",
    "    \n",
    "    # Predictions for undersampled test set\n",
    "    undersampled_test_preds = model.predict(X_Test_Undersampled)\n",
    "    undersampled_test_probs = model.predict_proba(X_Test_Undersampled)[:, 1]  # Probability estimates\n",
    "    \n",
    "    # Calculate accuracy and F1 score for oversampled data\n",
    "    oversampled_test_acc = accuracy_score(Y_Test, oversampled_test_preds)\n",
    "    oversampled_test_f1 = f1_score(Y_Test, oversampled_test_preds)\n",
    "    oversampled_test_auc = roc_auc_score(Y_Test, oversampled_test_probs)  # AUC Score\n",
    "    \n",
    "    # Calculate accuracy and F1 score for undersampled data\n",
    "    undersampled_test_acc = accuracy_score(Y_Test, undersampled_test_preds)\n",
    "    undersampled_test_f1 = f1_score(Y_Test, undersampled_test_preds)\n",
    "    undersampled_test_auc = roc_auc_score(Y_Test, undersampled_test_probs)  # AUC Score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print results\n",
    "for name, model in models.items():\n",
    "    print(f\"Model: {name}\")\n",
    "    print(f\"--- Oversampled Data ---\")\n",
    "    print(f\"Test Accuracy: {oversampled_test_acc:.4f}\")\n",
    "    print(f\"Test F1 Score: {oversampled_test_f1:.4f}\")\n",
    "    print(f\"Test AUC Score: {oversampled_test_auc:.4f}\")\n",
    "    \n",
    "    print(f\"--- Undersampled Data ---\")\n",
    "    print(f\"Test Accuracy: {undersampled_test_acc:.4f}\")\n",
    "    print(f\"Test F1 Score: {undersampled_test_f1:.4f}\")\n",
    "    print(f\"Test AUC Score: {undersampled_test_auc:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

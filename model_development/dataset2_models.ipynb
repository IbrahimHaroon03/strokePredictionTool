{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading Imports & the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the csv data file to a Pandas DataFrame\n",
    "dataset = pd.read_csv('../datasets/dataset2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataset Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Finding the total rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the total rows and columns\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Identifying data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â Printing the first 5 rows of the dataset\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Checking for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if there are any missing values in each column\n",
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Checking if the distribution of stroke is balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Diagnosis'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocessing 1: Imputation & Oversampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Creating a copy of the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetCopy = dataset.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Removing the id & patient name column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetCopy = datasetCopy.drop('Patient ID', axis=1)\n",
    "datasetCopy = datasetCopy.drop('Patient Name', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Replacing missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetCopy[\"Symptoms\"].fillna(\"No Symptoms\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. One Hot Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetCopy = pd.get_dummies(datasetCopy, columns=['Marital Status', 'Work Type', 'Smoking Status', 'Alcohol Intake', 'Physical Activity', 'Dietary Habits'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert boolean columns to 0 and 1\n",
    "datasetCopy[[\n",
    "    'Marital Status_Divorced', 'Marital Status_Married', 'Marital Status_Single',\n",
    "    'Work Type_Government Job', 'Work Type_Never Worked', 'Work Type_Private', 'Work Type_Self-employed',\n",
    "    'Smoking Status_Currently Smokes', 'Smoking Status_Formerly Smoked', 'Smoking Status_Non-smoker',\n",
    "    'Alcohol Intake_Frequent Drinker', 'Alcohol Intake_Never', 'Alcohol Intake_Rarely', 'Alcohol Intake_Social Drinker',\n",
    "    'Physical Activity_High', 'Physical Activity_Low', 'Physical Activity_Moderate',\n",
    "    'Dietary Habits_Gluten-Free', 'Dietary Habits_Keto', 'Dietary Habits_Non-Vegetarian', 'Dietary Habits_Paleo',\n",
    "    'Dietary Habits_Pescatarian', 'Dietary Habits_Vegan', 'Dietary Habits_Vegetarian'\n",
    "]] = datasetCopy[[\n",
    "    'Marital Status_Divorced', 'Marital Status_Married', 'Marital Status_Single',\n",
    "    'Work Type_Government Job', 'Work Type_Never Worked', 'Work Type_Private', 'Work Type_Self-employed',\n",
    "    'Smoking Status_Currently Smokes', 'Smoking Status_Formerly Smoked', 'Smoking Status_Non-smoker',\n",
    "    'Alcohol Intake_Frequent Drinker', 'Alcohol Intake_Never', 'Alcohol Intake_Rarely', 'Alcohol Intake_Social Drinker',\n",
    "    'Physical Activity_High', 'Physical Activity_Low', 'Physical Activity_Moderate',\n",
    "    'Dietary Habits_Gluten-Free', 'Dietary Habits_Keto', 'Dietary Habits_Non-Vegetarian', 'Dietary Habits_Paleo',\n",
    "    'Dietary Habits_Pescatarian', 'Dietary Habits_Vegan', 'Dietary Habits_Vegetarian'\n",
    "]].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Binary encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetCopy['Gender'] = datasetCopy['Gender'].replace({'Male' : 0, 'Female' : 1})\n",
    "datasetCopy['Residence Type'] = datasetCopy['Residence Type'].replace({'Rural' : 0, 'Urban' : 1})\n",
    "datasetCopy['Family History of Stroke'] = datasetCopy['Family History of Stroke'].replace({'Yes' : 0, 'No' : 1})\n",
    "datasetCopy['Diagnosis'] = datasetCopy['Diagnosis'].replace({'No Stroke' : 0, 'Stroke' : 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. MultiLabel Binarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Convert 'Symptoms' column to a list of individual symptoms\n",
    "datasetCopy[\"Symptoms\"] = datasetCopy[\"Symptoms\"].apply(lambda x: x.split(\", \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Apply MultiLabel Binarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "symptoms_encoded = pd.DataFrame(mlb.fit_transform(datasetCopy[\"Symptoms\"]), columns=mlb.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Merge the encoded symptoms back into the dataset and drop the original 'Symptoms' column\n",
    "datasetCopy = pd.concat([datasetCopy, symptoms_encoded], axis=1).drop(columns=[\"Symptoms\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "datasetCopy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Splitting blood pressure and cholesterol columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetCopy[['Systolic', 'Diastolic']] = datasetCopy['Blood Pressure Levels'].str.split('/', expand=True).astype(float)\n",
    "datasetCopy = datasetCopy.drop('Blood Pressure Levels', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetCopy['HDL'] = datasetCopy['Cholesterol Levels'].str.extract(r'HDL:\\s*(\\d+)').astype(float)\n",
    "datasetCopy['LDL'] = datasetCopy['Cholesterol Levels'].str.extract(r'LDL:\\s*(\\d+)').astype(float)\n",
    "datasetCopy = datasetCopy.drop('Cholesterol Levels', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Splitting the dataset into features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = datasetCopy.drop(columns='Diagnosis', axis=1)\n",
    "y = datasetCopy['Diagnosis']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Train - test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Train, X_Test, Y_Train, Y_Test = train_test_split(x, y, train_size=0.7, shuffle=True, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Scaling values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(X_Train)\n",
    "X_Train= pd.DataFrame(scaler.transform(X_Train), index=X_Train.index, columns=X_Train.columns)\n",
    "X_Test = pd.DataFrame(scaler.transform(X_Test), index=X_Test.index, columns=X_Test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking standard Deviation\n",
    "print(X_Train.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"                   Logistic Regression\": LogisticRegression(),\n",
    "    \"                   K-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"                         Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"                        Neural Network\": MLPClassifier(),\n",
    "    \"                         Random Forest\": RandomForestClassifier(),\n",
    "    \"                     Gradient Boosting\": GradientBoostingClassifier(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Loop through the models\n",
    "for name, model in models.items():\n",
    "    # Train the model\n",
    "    model.fit(X_Train, Y_Train)\n",
    "    \n",
    "    # Get binary predictions (not probabilities)\n",
    "    Y_pred = model.predict(X_Test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    test_acc = accuracy_score(Y_Test, Y_pred)\n",
    "    test_f1 = f1_score(Y_Test, Y_pred)\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        'accuracy': test_acc,\n",
    "        'f1': test_f1\n",
    "    }\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print results\n",
    "for name, model in models.items():\n",
    "    print(f\"Model: {name}\")\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"Test F1 Score: {test_f1:.4f}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
